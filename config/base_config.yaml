base_model:
  model_name: NousResearch/Llama-2-7b-hf
  device_map: auto
  max_length: 128
  batch_size: 64
  mirco_batch_size: 16
  gradient_accumulation_steps: 4

lora_model:
  r: 8
  lora_alpha: 32
  lora_dropout: 0.1
  bias: none
  task_type: CASUAL_LM

dataset:
  name: databricks/databricks-dolly-15k
  split: train

train_config:
  output_dir: ./llama-7b-int4-dolly
  num_train_epochs: 20
  max_steps: 200
  fp16: True
  optim: paged_adamw_32bit
  learning_rate: 2e-4
  lr_scheduler_type: constant
  per_device_train_batch_size: 16
  gradient_accumulation_steps: 4
  gradient_checkpointing: True
  group_by_length: False
  logging_steps: 10
  save_strategy: epoch
  save_total_limit: 3
  disable_tqdm: False




